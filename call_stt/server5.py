from flask import Flask, render_template
from flask_sock import Sock
import json
import base64
import audioop
import wave
from datetime import datetime
import os
import sys
import numpy as np
import torch
import librosa
import soundfile as sf

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(BASE_DIR)

# Denoiser ê²½ë¡œ ì¶”ê°€
denoiser_directory = os.path.join(BASE_DIR, 'src', 'denoiser')
sys.path.append(denoiser_directory)

from denoiser import pretrained
import nemo.collections.asr as nemo_asr
from transformers import AutoModelForCausalLM, AutoTokenizer
import re
import unicodedata

# Beam Search + LM ë””ì½”ë” (pyctcdecode)
try:
    from pyctcdecode import build_ctcdecoder
    import kenlm
    HAS_PYCTCDECODE = True
except ImportError:
    HAS_PYCTCDECODE = False
    print("âš ï¸ pyctcdecode ë˜ëŠ” kenlmì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Greedy ë””ì½”ë”©ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# ì„¤ì • íŒŒì¼ import
try:
    from config import (
        HTTP_SERVER_PORT,
        SAMPLE_RATE_INPUT,
        SAMPLE_RATE_TARGET,
        CHUNK_DURATION,
        DENOISER_MODEL_PATH,
        ASR_MODEL_PATH,
        KEYWORD_MODEL_PATH,
        RECORDINGS_DIR
    )
except ImportError:
    # config.pyê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
    HTTP_SERVER_PORT = 5000
    SAMPLE_RATE_INPUT = 8000
    SAMPLE_RATE_TARGET = 16000
    CHUNK_DURATION = 2.0
    DENOISER_MODEL_PATH = os.path.join(BASE_DIR, 'models', 'denoiser.th')
    ASR_MODEL_PATH = os.path.join(BASE_DIR, 'models', 'Conformer-CTC-BPE.nemo')
    KEYWORD_MODEL_PATH = os.path.join(BASE_DIR, 'models', 'qwen3-1.7b')
    RECORDINGS_DIR = os.path.join(BASE_DIR, 'call_recordings')

app = Flask(__name__)
sock = Sock(app)

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ ì €ì¥
denoiser_model = None
asr_model = None
keyword_model = None
keyword_tokenizer = None
device = None
ctc_decoder = None  # Beam Search + LM ë””ì½”ë”
USE_BEAM_SEARCH = False  # Beam Search ì‚¬ìš© ì—¬ë¶€
BEAM_WIDTH = 10  # Beam í¬ê¸°

def log(msg, *args):
    print(f"Media WS: ", msg, *args)

def load_models():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    global denoiser_model, asr_model, keyword_model, keyword_tokenizer, device, ctc_decoder, USE_BEAM_SEARCH
    
    log("Loading models...")
    
    # Device ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    log(f"Using device: {device}")
    
    # Denoiser ëª¨ë¸ ë¡œë“œ
    try:
        import argparse
        denoiser_args = argparse.Namespace(
            dns64=False,
            dns48=False,
            master64=False,
            device=str(device),
            dry=0.04,
            model_path=DENOISER_MODEL_PATH
        )
        denoiser_model = pretrained.get_model(denoiser_args).to(device)
        denoiser_model.eval()
        log("âœ“ Denoiser model loaded successfully")
    except Exception as e:
        log(f"Warning: Could not load denoiser model: {e}")
        denoiser_model = None
    
    # ASR ëª¨ë¸ ë¡œë“œ
    try:
        asr_model = nemo_asr.models.EncDecCTCModelBPE.restore_from(ASR_MODEL_PATH, map_location=device)
        asr_model.eval()
        
        # Preprocessor ì„¤ì •
        from omegaconf import OmegaConf
        import copy
        asr_cfg = copy.deepcopy(asr_model._cfg)
        OmegaConf.set_struct(asr_cfg.preprocessor, False)
        asr_cfg.preprocessor.dither = 0.0
        asr_cfg.preprocessor.pad_to = 0
        OmegaConf.set_struct(asr_cfg.preprocessor, True)
        asr_model.preprocessor = asr_model.from_config_dict(asr_cfg.preprocessor)
        
        if device.type == 'cuda':
            asr_model.cuda()
        
        log("âœ“ ASR model loaded successfully")
        
        # Vocabulary ë¡œë“œ (pyctcdecodeì— í•„ìš”)
        vocab_list = None
        try:
            vocab_path = os.path.join(BASE_DIR, 'src', 'nemo_asr', 'tokenizer_spe_bpe_v2048', 'vocab.txt')
            with open(vocab_path, 'r', encoding='utf-8') as f:
                vocab_list = [line.strip() for line in f]
            log(f"âœ“ Loaded vocabulary: {len(vocab_list)} tokens")
        except Exception as e:
            log(f"Warning: Could not load vocabulary: {e}")
            vocab_list = None
        
        # Beam Search + LM ë””ì½”ë” ì„¤ì • (pyctcdecode ë°©ì‹)
        if HAS_PYCTCDECODE and vocab_list:
            try:
                # KenLM ëª¨ë¸ ê²½ë¡œ íƒìƒ‰ (ìš°ì„ ìˆœìœ„ ìˆœ)
                kenlm_paths = [
                    os.path.join(BASE_DIR, 'models', 'korean_4gram.binary'),  # ìƒˆë¡œ í•™ìŠµí•œ ëª¨ë¸
                    os.path.join(BASE_DIR, 'models', 'korean_4gram.arpa'),    # ARPA ë²„ì „
                ]
                
                kenlm_model_path = None
                for path in kenlm_paths:
                    if os.path.exists(path):
                        kenlm_model_path = path
                        log(f"âœ“ Found KenLM model: {kenlm_model_path}")
                        break
                
                if kenlm_model_path:
                    # KenLM ëª¨ë¸ ë¡œë“œ
                    kenlm_model = kenlm.Model(kenlm_model_path)
                    log("âœ“ KenLM model loaded successfully")
                    
                    # pyctcdecode decoder ìƒì„±
                    ctc_decoder = build_ctcdecoder(
                        labels=vocab_list,
                        kenlm_model=kenlm_model,
                        alpha=0.5,  # LM weight (íŠœë‹ ê°€ëŠ¥)
                        beta=1.0,   # Word insertion bonus (íŠœë‹ ê°€ëŠ¥)
                    )
                    
                    USE_BEAM_SEARCH = True
                    log("âœ… pyctcdecode + KenLM decoder configured successfully")
                    log(f"   - Method: pyctcdecode (Python-based, Windows compatible)")
                    log(f"   - Model: {os.path.basename(kenlm_model_path)}")
                    log(f"   - Vocabulary size: {len(vocab_list)}")
                    log(f"   - Alpha (LM weight): 0.5")
                    log(f"   - Beta (word bonus): 1.0")
                    log(f"   - Language model trained on 93,723 Korean sentences")
                else:
                    log("âš  KenLM model not found in:")
                    for path in kenlm_paths:
                        log(f"   - {path}")
                    log("   Using Greedy decoding (without LM)")
                    USE_BEAM_SEARCH = False
                    
            except Exception as e:
                log(f"Warning: Could not configure KenLM: {e}")
                import traceback
                traceback.print_exc()
                USE_BEAM_SEARCH = False
        else:
            if not HAS_PYCTCDECODE:
                log("âš  pyctcdecode not available. Install with: pip install pyctcdecode")
            if not vocab_list:
                log("âš  Vocabulary not loaded")
            log("   Using Greedy decoding (without LM)")
            USE_BEAM_SEARCH = False
            
    except Exception as e:
        log(f"Warning: Could not load ASR model: {e}")
        asr_model = None
        ctc_decoder = None
        USE_BEAM_SEARCH = False
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ ë¡œë“œ (Qwen3-1.7B)
    try:
        if os.path.exists(KEYWORD_MODEL_PATH):
            # ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©
            keyword_model_path = KEYWORD_MODEL_PATH
            log(f"Loading keyword extraction model from local: {KEYWORD_MODEL_PATH}")
        else:
            # HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œ
            keyword_model_path = "Qwen/Qwen3-1.7B"
            log(f"Local model not found. Downloading from HuggingFace: {keyword_model_path}")
        
        keyword_tokenizer = AutoTokenizer.from_pretrained(keyword_model_path)
        keyword_model = AutoModelForCausalLM.from_pretrained(
            keyword_model_path,
            torch_dtype="auto",
            device_map="auto"
        )
        log("âœ“ Keyword extraction model loaded successfully")
        log(f"  - Model source: {'Local' if os.path.exists(KEYWORD_MODEL_PATH) else 'HuggingFace'}")
        log(f"  - Model path: {keyword_model_path}")
    except Exception as e:
        log(f"Warning: Could not load keyword model: {e}")
        keyword_model = None
        keyword_tokenizer = None
    
    log("All models loaded and ready!")

@app.route("/", methods=["GET"])
def index():
    return "OK", 200

@app.route('/twiml', methods=['GET', 'POST'])
def return_twiml():
    print("POST TwiML")
    return render_template('streams.xml')

@sock.route("/stream")
def echo(ws):
    log("Connection accepted")
    count = 0
    has_seen_media = False
    
    # í™”ìë³„ ì´ì¤‘ ë²„í¼ êµ¬ì¡°
    buffers = {
        'inbound': {  # ê³ ê°
            'audio': [],           # ì €ì¥ìš© ë²„í¼
            'processing': [],      # ì‹¤ì‹œê°„ ì²˜ë¦¬ìš© ë²„í¼
            'transcriptions': [],  # ì „ì‚¬ ê²°ê³¼
            'keywords': []         # ì¶”ì¶œëœ í‚¤ì›Œë“œ
        },
        'outbound': {  # ìƒë‹´ì‚¬
            'audio': [],
            'processing': [],
            'transcriptions': [],
            'keywords': []
        }
    }
    
    # ì²˜ë¦¬ íŒŒë¼ë¯¸í„°
    CHUNK_SIZE = int(SAMPLE_RATE_INPUT * CHUNK_DURATION)  # ìƒ˜í”Œ ìˆ˜
    
    # í™”ì ë¼ë²¨ ë§¤í•‘
    speaker_labels = {
        'inbound': 'ê³ ê°',
        'outbound': 'ìƒë‹´ì‚¬'
    }
    
    while True:
        try:
            message = ws.receive()
            if message is None:
                log("No message received...")
                break
            
            data = json.loads(message)
            
            if data['event'] == "connected":
                log("Connected Message received")
                
            if data['event'] == "start":
                log("Start Message received")
                log("Starting real-time dual-track Denoise + STT processing...")
                log("Track: inbound (ê³ ê°) / outbound (ìƒë‹´ì‚¬)")
                
            if data['event'] == "media":
                if not has_seen_media:
                    log("Media messages received - processing started")
                    has_seen_media = True
                
                # track í•„ë“œë¡œ í™”ì êµ¬ë¶„
                track = data['media'].get('track', 'inbound_track')
                
                # ë””ë²„ê¹…: track ê°’ í™•ì¸ (ì²˜ìŒ ëª‡ ê°œë§Œ ì¶œë ¥)
                if count < 5:
                    log(f"DEBUG: Received track value: '{track}'")
                
                # track ê°’ì— ë”°ë¼ í™”ì êµ¬ë¶„
                if 'inbound' in track.lower():
                    speaker = 'inbound'
                elif 'outbound' in track.lower():
                    speaker = 'outbound'
                else:
                    # ê¸°ë³¸ê°’ì€ inboundë¡œ ì„¤ì •
                    speaker = 'inbound'
                    if count < 5:
                        log(f"WARNING: Unknown track value '{track}', defaulting to inbound")
                
                # base64 ë””ì½”ë”©
                payload = data['media']['payload']
                audio_data = base64.b64decode(payload)
                
                # mu-lawë¥¼ PCMìœ¼ë¡œ ë³€í™˜ (8bit mu-law -> 16bit PCM)
                pcm_data = audioop.ulaw2lin(audio_data, 2)
                
                # í•´ë‹¹ í™”ìì˜ ë²„í¼ì— ì¶”ê°€
                buffers[speaker]['audio'].append(pcm_data)
                buffers[speaker]['processing'].append(pcm_data)
                
                # ë²„í¼ê°€ ì¶©ë¶„íˆ ìŒ“ì´ë©´ ì²˜ë¦¬
                current_size = sum(len(chunk) for chunk in buffers[speaker]['processing'])
                if current_size >= CHUNK_SIZE * 2:  # 16-bit = 2 bytes per sample
                    # ì‹¤ì‹œê°„ ì²˜ë¦¬
                    try:
                        transcription = process_audio_chunk(
                            buffers[speaker]['processing'], 
                            SAMPLE_RATE_INPUT, 
                            SAMPLE_RATE_TARGET
                        )
                        if transcription:
                            buffers[speaker]['transcriptions'].append(transcription)
                            log(f"[{speaker_labels[speaker]}] Transcription: {transcription}")
                            
                            # í‚¤ì›Œë“œ ì¶”ì¶œ
                            keywords = extract_keywords(transcription)
                            if keywords:
                                buffers[speaker]['keywords'].extend(keywords)
                                log(f"[{speaker_labels[speaker]}] ğŸ”‘ Keywords: {keywords}")
                    except Exception as e:
                        log(f"[{speaker_labels[speaker]}] Error processing chunk: {e}")
                    
                    # ì²˜ë¦¬ìš© ë²„í¼ ì´ˆê¸°í™”
                    buffers[speaker]['processing'] = []
                
            if data['event'] == "closed":
                log("Closed Message received")
                break
                
            count += 1
            
        except Exception as e:
            log(f"Error: {e}")
            import traceback
            traceback.print_exc()
            break

    log(f"Connection closed. Received a total of {count} messages")
    
    # ë‚¨ì€ ë²„í¼ ì²˜ë¦¬ (ì–‘ìª½ í™”ì ëª¨ë‘)
    for speaker in ['inbound', 'outbound']:
        if buffers[speaker]['processing']:
            try:
                transcription = process_audio_chunk(
                    buffers[speaker]['processing'], 
                    SAMPLE_RATE_INPUT, 
                    SAMPLE_RATE_TARGET
                )
                if transcription:
                    buffers[speaker]['transcriptions'].append(transcription)
                    log(f"[{speaker_labels[speaker]}] Final transcription: {transcription}")
                    
                    # ë§ˆì§€ë§‰ í‚¤ì›Œë“œ ì¶”ì¶œ
                    keywords = extract_keywords(transcription)
                    if keywords:
                        buffers[speaker]['keywords'].extend(keywords)
                        log(f"[{speaker_labels[speaker]}] ğŸ”‘ Keywords: {keywords}")
            except Exception as e:
                log(f"[{speaker_labels[speaker]}] Error processing final chunk: {e}")
    
    # í™”ìë³„ íŒŒì¼ ì €ì¥
    save_dual_track_results(buffers, speaker_labels)

def extract_keywords(text):
    """
    Qwen3-1.7Bë¥¼ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ ë¬¸ì¥ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    
    Args:
        text: í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  í•œêµ­ì–´ ë¬¸ì¥
        
    Returns:
        list: ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    if not text or not text.strip() or keyword_model is None or keyword_tokenizer is None:
        return []
    
    try:
        system_prompt = (
            "ë‹¹ì‹ ì€ í•œêµ­ì–´ í•œ ë¬¸ì¥ì—ì„œ ê²€ìƒ‰/ë¶„ë¥˜ì— ìœ ì˜ë¯¸í•œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n"
            "ê·œì¹™:\n"
            "- í‚¤ì›Œë“œëŠ” ê³ ìœ ëª…ì‚¬, ê¸°ìˆ ëª…, ê°œë…, ê°ì²´ ì¤‘ì‹¬\n"
            "- ê°ì •, ì¶”ì„ìƒˆ, ì¼ë°˜ì ì¸ ë§ì€ ì œì™¸\n"
            "- í‚¤ì›Œë“œê°€ í•„ìš” ì—†ìœ¼ë©´ ë°˜ë“œì‹œ ë¹ˆ ë°°ì—´ì„ ë°˜í™˜\n"
            "- ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON í•œ ì¤„ë¡œë§Œ: {\"keywords\": [..]}\n"
            "- ì¶”ë¡  ê³¼ì •, ì„¤ëª…, ì¶”ê°€ ë¬¸ì¥ ê¸ˆì§€\n"
        )

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"ë¬¸ì¥: {text}"}
        ]

        text_input = keyword_tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False
        )

        model_inputs = keyword_tokenizer([text_input], return_tensors="pt").to(keyword_model.device)

        generated_ids = keyword_model.generate(
            **model_inputs,
            max_new_tokens=128,
            min_new_tokens=5,
            do_sample=True,
            temperature=0.7,
            top_p=0.8,
            top_k=20,
            pad_token_id=keyword_tokenizer.eos_token_id
        )

        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):]
        decoded_text = keyword_tokenizer.decode(output_ids, skip_special_tokens=True).strip()

        # think íƒœê·¸ ì œê±°
        decoded_text = re.sub(r'<think>.*?</think>', '', decoded_text, flags=re.DOTALL).strip()

        # JSON ì¶”ì¶œ
        m = re.search(r'\{.*\}', decoded_text, flags=re.DOTALL)
        if not m:
            return []

        result = json.loads(m.group(0))
        return result.get('keywords', [])
        
    except Exception as e:
        log(f"Error in extract_keywords: {e}")
        return []

def process_audio_chunk(buffer, input_sr, target_sr):
    """ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ Denoise + STT ì²˜ë¦¬"""
    try:
        # ë²„í¼ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        audio_data = b''.join(buffer)
        audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
        
        # ë¦¬ìƒ˜í”Œë§ (8kHz -> 16kHz)
        if input_sr != target_sr:
            audio_resampled = librosa.resample(audio_np, orig_sr=input_sr, target_sr=target_sr)
        else:
            audio_resampled = audio_np
        
        # Denoise
        if denoiser_model is not None:
            audio_tensor = torch.tensor(audio_resampled).unsqueeze(0).unsqueeze(0).to(device)
            with torch.no_grad():
                audio_denoised = denoiser_model(audio_tensor)
            audio_denoised = audio_denoised.squeeze().cpu().numpy()
        else:
            audio_denoised = audio_resampled
        
        # STT
        if asr_model is not None:
            with torch.no_grad():
                if USE_BEAM_SEARCH and ctc_decoder is not None:
                    # pyctcdecode ì‚¬ìš©: logits ì¶”ì¶œ í›„ ë””ì½”ë”©
                    try:
                        # audioë¥¼ tensorë¡œ ë³€í™˜
                        audio_tensor = torch.tensor(audio_denoised).unsqueeze(0).to(device)
                        audio_length = torch.tensor([audio_tensor.shape[1]]).to(device)
                        
                        # NeMo ëª¨ë¸ì—ì„œ logits ì¶”ì¶œ
                        processed_signal, processed_signal_length = asr_model.preprocessor(
                            input_signal=audio_tensor, length=audio_length
                        )
                        if asr_model.spec_augmentation is not None and asr_model.training:
                            processed_signal = asr_model.spec_augmentation(
                                input_spec=processed_signal, length=processed_signal_length
                            )
                        encoded, encoded_len = asr_model.encoder(
                            audio_signal=processed_signal, length=processed_signal_length
                        )
                        log_probs = asr_model.decoder(encoder_output=encoded)
                        
                        # pyctcdecodeë¡œ ë””ì½”ë”©
                        # log_probs shape: [batch=1, time, vocab]
                        logits_np = log_probs[0].cpu().numpy()  # [time, vocab]
                        text = ctc_decoder.decode(logits_np)
                        
                        if text:
                            text = unicodedata.normalize('NFC', text)
                            return text.strip()
                    except Exception as e:
                        log(f"pyctcdecode failed, falling back to Greedy: {e}")
                        # Beam Search ì‹¤íŒ¨ ì‹œ Greedy ë””ì½”ë”©ìœ¼ë¡œ í´ë°±
                
                # Greedy ë””ì½”ë”© (ê¸°ì¡´ ë°©ì‹ ë˜ëŠ” í´ë°±)
                transcription = asr_model.transcribe([audio_denoised], batch_size=1)
                if transcription and len(transcription) > 0:
                    # Hypothesis ê°ì²´ì—ì„œ text ì†ì„± ì¶”ì¶œ
                    result = transcription[0]
                    if hasattr(result, 'text'):
                        text = result.text
                    else:
                        text = str(result)
                    
                    if text:
                        text = unicodedata.normalize('NFC', text)
                        return text.strip()
        
        return None
        
    except Exception as e:
        log(f"Error in process_audio_chunk: {e}")
        import traceback
        traceback.print_exc()
        return None

def save_dual_track_results(buffers, speaker_labels):
    """í™”ìë³„ ì˜¤ë””ì˜¤, ì „ì‚¬ ê²°ê³¼, í‚¤ì›Œë“œ ì €ì¥"""
    try:
        # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ íŒŒì¼ëª…
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        os.makedirs(RECORDINGS_DIR, exist_ok=True)
        
        # íŒŒì¼ëª… ë§¤í•‘
        file_suffixes = {
            'inbound': 'customer',   # ê³ ê°
            'outbound': 'agent'      # ìƒë‹´ì‚¬
        }
        
        total_duration = 0
        stats = {}
        
        # ê° í™”ìë³„ë¡œ íŒŒì¼ ì €ì¥
        for speaker in ['inbound', 'outbound']:
            suffix = file_suffixes[speaker]
            label = speaker_labels[speaker]
            
            # ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì €ì¥
            if buffers[speaker]['audio']:
                # WAV íŒŒì¼ ì €ì¥
                audio_filename = os.path.join(RECORDINGS_DIR, f"call_{timestamp}_{suffix}.wav")
                audio_data = b''.join(buffers[speaker]['audio'])
                
                with wave.open(audio_filename, 'wb') as wav_file:
                    wav_file.setnchannels(1)
                    wav_file.setsampwidth(2)
                    wav_file.setframerate(SAMPLE_RATE_INPUT)
                    wav_file.writeframes(audio_data)
                
                duration = len(audio_data) / (SAMPLE_RATE_INPUT * 2)
                total_duration = max(total_duration, duration)
                log(f"[{label}] Audio saved: {audio_filename}")
                log(f"[{label}] Duration: {duration:.2f} seconds")
                
                # í†µê³„ ì €ì¥
                stats[speaker] = {
                    'audio_file': audio_filename,
                    'duration': duration,
                    'chunks': len(buffers[speaker]['audio'])
                }
            
            # ì „ì‚¬ ê²°ê³¼ ë° í‚¤ì›Œë“œ ì €ì¥
            if buffers[speaker]['transcriptions']:
                txt_filename = os.path.join(RECORDINGS_DIR, f"call_{timestamp}_{suffix}.txt")
                with open(txt_filename, 'w', encoding='utf-8') as f:
                    f.write(f"=== í™”ì: {label} ({speaker.capitalize()} Track) ===\n\n")
                    
                    f.write("=== Real-time Transcription Results ===\n\n")
                    for i, trans in enumerate(buffers[speaker]['transcriptions'], 1):
                        f.write(f"[Chunk {i}] {trans}\n")
                    
                    f.write("\n=== Full Transcription ===\n")
                    full_text = " ".join(buffers[speaker]['transcriptions'])
                    f.write(full_text)
                    
                    # í‚¤ì›Œë“œ ì¶”ê°€
                    if buffers[speaker]['keywords']:
                        f.write("\n\n=== Extracted Keywords ===\n")
                        unique_keywords = list(set(buffers[speaker]['keywords']))
                        f.write(f"Total unique keywords: {len(unique_keywords)}\n")
                        f.write(f"Keywords: {', '.join(unique_keywords)}\n")
                
                log(f"[{label}] Transcription saved: {txt_filename}")
                log(f"[{label}] Total chunks transcribed: {len(buffers[speaker]['transcriptions'])}")
                
                if buffers[speaker]['keywords']:
                    unique_keywords = list(set(buffers[speaker]['keywords']))
                    log(f"[{label}] Extracted {len(unique_keywords)} unique keywords: {unique_keywords}")
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                if speaker in stats:
                    stats[speaker]['txt_file'] = txt_filename
                    stats[speaker]['transcriptions'] = len(buffers[speaker]['transcriptions'])
                    stats[speaker]['keywords'] = len(unique_keywords) if buffers[speaker]['keywords'] else 0
        
        # ì „ì²´ í†µí™” ìš”ì•½
        log("\n=== Call Summary ===")
        log(f"Total call duration: {total_duration:.2f} seconds")
        for speaker in ['inbound', 'outbound']:
            if speaker in stats:
                label = speaker_labels[speaker]
                log(f"[{label}] Chunks: {stats[speaker].get('chunks', 0)}, "
                    f"Transcriptions: {stats[speaker].get('transcriptions', 0)}, "
                    f"Keywords: {stats[speaker].get('keywords', 0)}")
        
    except Exception as e:
        log(f"Error saving results: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    # ëª¨ë¸ ë¡œë“œ
    load_models()
    
    # ì„œë²„ ì‹œì‘
    log("Starting server...")
    log(f"Server will listen on port {HTTP_SERVER_PORT}")
    app.run(host='0.0.0.0', port=HTTP_SERVER_PORT, debug=True, use_reloader=False)
